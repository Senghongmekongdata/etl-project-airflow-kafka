# ***Project 1: Creating Streaming Data Pipelines using Kafka***


![download](https://user-images.githubusercontent.com/58208161/182288549-bf4c7e47-07d4-427d-8df4-0de840f2e6bc.png)


## Scenario
You are a data engineer at a data analytics consulting company. You have been assigned to a project that aims to de-congest the national highways by analyzing the road traffic data from different toll plazas. As a vehicle passes a toll plaza, the vehicle's data like `vehicle_id,vehicle_type,toll_plaza_id` and `timestamp` are streamed to Kafka. Your job is to create a data pipe line that collects the streaming data and loads it into a database.


## Objectives
In this assignment you will create a streaming data pipe by performing these steps:

- Start a MySQL Database server.
- Create a table to hold the toll data.
- Start the Kafka server.
- Install the Kafka python driver.
- Install the MySQL python driver.
- Create a topic named toll in kafka.
- Download streaming data generator program.
- Customize the generator program to steam to toll topic.
- Download and customise streaming data consumer.
- Customize the consumer program to write into a MySQL database table.
- Verify that streamed data is being collected in the database table.

> Note:  Make sure you are samilar with bash shell and some basic knownledge on kafka and running on ubuntu or linux machine and have mysql running in this machine as well as python.

## Prepare the lab environment
- Step 1: Download Kafka
```
wget https://archive.apache.org/dist/kafka/2.8.0/kafka_2.12-2.8.0.tgz
```
- Step 2: Extract Kafka 
```
tar -xzf kafka_2.12-2.8.0.tgz
```
- Step 3: Start MySQL server.
> If you are running on wsl ubuntu machine check this doc: https://docs.microsoft.com/en-us/windows/wsl/tutorials/wsl-database
```
start_mysql
```
- Step 4: Connect to the mysql server, using the command below. Make sure you use the password given to you when the MySQL server starts. Please make a note or record of the password because you will need it later.
```
mysql --host=127.0.0.1 --port=3306 --user=root --password=Mjk0NDQtcnNhbm5h
```
- Step 5: Create a database named `tolldata`
> At the 'mysql>' prompt, run the command below to create the database.
```
create database tolldata;
```
- Step 6: Create a table named livetolldata with the schema to store the data generated by the traffic simulator.
> Run the following command to create the table:
```
use tolldata;

create table livetolldata(timestamp datetime,vehicle_id int,vehicle_type char(15),toll_plaza_id smallint);
```
This is the table where you would store all the streamed data that comes from kafka. Each row is a record of when a vehicle has passed through a certain toll plaza along with its type and anonymized id.

- Step 7: Disconnect from MySQL server.
```
exit
```
- Step 8: Install the python module kafka-python using the pip3 command.
```
pip3 install kafka-python
```
This python module will help you to communicate with kafka server. It can used to send and receive messages from kafka.

- Step 8: Install the python module mysql-connector-python using the pip3 command.
```
pip3 install mysql-connector-python 
```
This python module will help you to interact with mysql server.

## Start Kafka
### Task 2.1 - Start Zookeeper
Start zookeeper server.

Take a screenshot of the command you run.

### Task 2.2 - Start Kafka server
Start Kafka server

### Task 2.3 - Create a topic named toll
Create a Kakfa topic named `toll`


### Task 2.4 - Download the Toll Traffic Simulator
Download the `toll_traffic_generator.py` from the url given below using 'wget'.

`https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Final%20Assignment/toll_traffic_generator.py`

Open the code using the theia editor using the "Menu --> File -->Open" option.

### Task 2.5 - Configure the Toll Traffic Simulator
Open the `toll_traffic_generator.py` and set the topic to toll.

### Task 2.6 - Run the Toll Traffic Simulator
Run the `toll_traffic_generator.py`.

Hint : python3 <pythonfilename> runs a python program on the theia lab.

### Task 2.7 - Configure streaming_data_reader.py
Download the `streaming_data_reader.py` from the url below using 'wget'.

`
https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Final%20Assignment/streaming_data_reader.py`

Open the streaming_data_reader.py and modify the following details so that the program can connect to your mysql server.

**TOPIC**

**DATABASE**

**USERNAME**

**PASSWORD**

### Task 2.8 - Run streaming_data_reader.py
Run the `streaming_data_reader.py`
```
python3 streaming_data_reader.py
```

### Task 2.9 - Health check of the streaming data pipeline.
If you have done all the steps till here correctly, the streaming toll data would get stored in the table livetolldata.

List the top 10 rows in the table livetolldata to see whether you get the right data or not.

***Congratulation for your first project***
----

<br  />

<br  />

# ***Project 2: Creating ETL Data Pipelines using Apache Airflow***

![download](https://user-images.githubusercontent.com/58208161/182294225-1e490d3c-10b2-46eb-b890-dac7673ee283.png)


## Scenario
You are a data engineer at a data analytics consulting company. You have been assigned to a project that aims to de-congest the national highways by analyzing the road traffic data from different toll plazas. Each highway is operated by a different toll operator with a different IT setup that uses different file formats. Your job is to collect data available in different formats and consolidate it into a single file.

## Objectives
In this assignment you will author an Apache Airflow DAG that will:

- Extract data from a csv file
- Extract data from a tsv file
- Extract data from a fixed width file
- Transform the data
- Load the transformed data into the staging area

## Prepare the lab environment

Before you start the assignment:

- Start Apache Airflow.
- Download the dataset from the source to the destination mentioned below.

> Source:
```
https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Final%20Assignment/tolldata.tgz
```

- Create a directory for staging area (You can create any path in your machine)
> In here, I create under this path 
```
mkdir /home/project/airflow/dags/finalassignment/staging
```

## Create a DAG

### Task 1.1 - Define DAG arguments
Define the DAG arguments as per the following details:

![image](https://user-images.githubusercontent.com/58208161/182295022-4d91cca3-a1ef-4b38-b4d3-dfb503589d29.png)

### Task 1.2 - Define the DAG
Create a DAG as per the following details.

![image](https://user-images.githubusercontent.com/58208161/182295212-1557f473-de36-48b9-b420-b5cb78a79f67.png)

### Task 1.3 - Create a task to unzip data
Create a task named `unzip_data`.

Use the downloaded data from the url given in the first part of this assignment in exercise 1 and uncompress it into the destination directory.

### Task 1.4 - Create a task to extract data from csv file
Create a task named `extract_data_from_csv`.

This task should extract the fields `Rowid, Timestamp, Anonymized Vehicle number, and Vehicle type` from the `vehicle-data.csv` file and save them into a file named `csv_data.csv`.


### Task 1.5 - Create a task to extract data from tsv file
Create a task named `extract_data_from_tsv`.

This task should extract the fields Number of axles, Tollplaza id, and Tollplaza code from the tollplaza-data.tsv file and save it into a file named tsv_data.csv.

### Task 1.6 - Create a task to extract data from fixed width file
Create a task named extract_data_from_fixed_width.

This task should extract the fields Type of Payment code, and Vehicle Code from the fixed width file payment-data.txt and save it into a file named fixed_width_data.csv.

### Task 1.7 - Create a task to consolidate data extracted from previous tasks
Create a task named consolidate_data.

This task should create a single csv file named extracted_data.csv by combining data from

- csv_data.csv
- tsv_data.csv
- fixed_width_data.csv
- The final csv file should use the fields in the order given below:

`Rowid, Timestamp, Anonymized Vehicle number, Vehicle type, Number of axles, Tollplaza id, Tollplaza code, Type of Payment code, and Vehicle Code`

Hint: Use the bash paste command.

paste command merges lines of files.

Example : paste file1 file2 > newfile

The above command merges the columns of the files file1 and file2 and sends the output to newfile.

You can use the command man paste to explore more.


### Task 1.8 - Transform and load the data
Create a task named transform_data.

This task should transform the vehicle_type field in extracted_data.csv into capital letters and save it into a file named transformed_data.csv in the staging directory. 

### Task 1.9 - Define the task pipeline
Define the task pipeline as per the details given below:

![image](https://user-images.githubusercontent.com/58208161/182296038-5f7b8140-4ea8-4283-87a1-00974db468c5.png)


## Getting the DAG operational
Save the DAG you defined into a file named `ETL_toll_data.py`.

### Task 1.10 - Submit the DAG

### Task 1.11 - Unpause the DAG

### Task 1.12 - Monitor the DAG

**Congratuation for your second project**

> Credit by Coursera
